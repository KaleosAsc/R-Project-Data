<<<<<<< Updated upstream
<<<<<<< Updated upstream
endpoint <- "https://api.cognitive.microsofttranslator.com/"
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
library(readr)
library(dplyr)
library(httr)
library(jsonlite)
library(purrr)
library(rlang)
library(cld2)
library(dotenv)
library(e1071)
library(caret)
library(tm)
multilingual_mobile_app_reviews_2025 <- read_csv("multilingual_mobile_app_reviews_2025_extended.csv")
View(multilingual_mobile_app_reviews_2025)
# Contar valores vac√≠os en todo el dataset
colSums(is.na(multilingual_mobile_app_reviews_2025))
Copia_Datos_Limpios <- multilingual_mobile_app_reviews_2025
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
select(-review_id, -num_helpful_votes, -user_id, -user_gender, -app_version)
colSums(is.na(Copia_Datos_Limpios))
# Filtro general para cualquier variable
filtro_user_country <- Copia_Datos_Limpios %>% filter(is.na(user_country))
filtro_review_text  <- Copia_Datos_Limpios %>% filter(is.na(review_text))
filtro_rating       <- Copia_Datos_Limpios %>% filter(is.na(rating))
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
mutate(
user_country = case_when(
review_language == "es" ~ "Spain",          # Spanish
review_language == "pt" ~ "Brazil",         # Portuguese
review_language == "ja" ~ "Japan",          # Japanese
review_language == "hi" ~ "India",          # Hindi
review_language == "ko" ~ "South Korea",    # Korean
review_language == "zh" ~ "China",          # Chinese
review_language == "de" ~ "Germany",        # German
review_language == "fr" ~ "France",         # French
review_language == "it" ~ "Italy",          # Italian
review_language == "ru" ~ "Russia",         # Russian
TRUE ~ user_country                         # Keep original if no match
)
)
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
filter(!is.na(rating))
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
filter(!is.na(review_text))
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
filter(!is.na(user_country))
colSums(is.na(Copia_Datos_Limpios)[, c("user_country", "review_text", "rating")])
sapply(Copia_Datos_Limpios,class)
language <- unique(Copia_Datos_Limpios$review_language)
typeof(language)
# Asegurar que review_language sea texto
Copia_Datos_Limpios$review_language <- as.character(Copia_Datos_Limpios$review_language)
# Reemplazar 'zh' por 'zh-Hans' (chino simplificado)
Copia_Datos_Limpios$review_language[Copia_Datos_Limpios$review_language == "zh"] <- "zh-Hans"
# Verificar valores √∫nicos despu√©s de la correcci√≥n
unique(Copia_Datos_Limpios$review_language)
translate_review_text <- function(text, target = "en", key, endpoint, region = "global") {
print(text)
if (is.na(text) || nchar(text) == 0) print("Value null")
url <- paste0(endpoint, "translate?api-version=3.0&to=", target)
body <- list(list(Text = text))
response <- POST(
url,
add_headers(
`Ocp-Apim-Subscription-Key` = key,
`Ocp-Apim-Subscription-Region` = region,
`Content-Type` = "application/json"
),
body = toJSON(body, auto_unbox = TRUE)
)
# Manejo de errores
if (status_code(response) != 200) {
warning(paste("Error:", content(response, "text", encoding = "UTF-8")))
return(NA_character_)
}
result <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
# cat("üì¶ Resultado bruto de la API:\n",
#   jsonlite::toJSON(result, pretty = TRUE, auto_unbox = TRUE),
#   "\n\n")
# str(result)
translated_text <- result$translations[[1]]$text
# cat("‚úÖ Traducido:", translated_text, "\n\n--- Fin impresi√≥n OK ---\n\n")
return(translated_text)
}
# --- Configuraci√≥n Azure ---
load_dot_env()
key <- Sys.getenv("AZURE_KEY")
endpoint <- "https://api.cognitive.microsofttranslator.com/"
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
# --- Configuraci√≥n Azure ---
load_dot_env()
key <- Sys.getenv("AZURE_KEY")
endpoint <- "https://api.cognitive.microsofttranslator.com/"
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
translate_review_text <- function(text, target = "en", key, endpoint, region = "global") {
print(text)
if (is.na(text) || nchar(text) == 0) print("Value null")
url <- paste0(endpoint, "translate?api-version=3.0&to=", target)
body <- list(list(Text = text))
response <- POST(
url,
add_headers(
`Ocp-Apim-Subscription-Key` = key,
`Ocp-Apim-Subscription-Region` = region,
`Content-Type` = "application/json"
),
body = toJSON(body, auto_unbox = TRUE)
)
# Manejo de errores
if (status_code(response) != 200) {
warning(paste("Error:", content(response, "text", encoding = "UTF-8")))
return(NA_character_)
}
result <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
# cat("üì¶ Resultado bruto de la API:\n",
#   jsonlite::toJSON(result, pretty = TRUE, auto_unbox = TRUE),
#   "\n\n")
# str(result)
translated_text <- result$translations[[1]]$text
# cat("‚úÖ Traducido:", translated_text, "\n\n--- Fin impresi√≥n OK ---\n\n")
return(translated_text)
}
# --- Configuraci√≥n Azure ---
load_dot_env()
key <- Sys.getenv("AZURE_KEY")
endpoint <- "https://api.cognitive.microsofttranslator.com/"
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
View(Copia_Datos_Limpios)
library(httr)
library(httr)
# --- Configuraci√≥n Azure ---
load_dot_env()
key <- Sys.getenv("AZURE_KEY")
endpoint <- "https://api.cognitive.microsofttranslator.com/"
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
<<<<<<< Updated upstream
<<<<<<< Updated upstream
boxplot(data_reviews$rating, main = "Boxplot de Rating")
View(Copia_Datos_Limpios)
View(multilingual_mobile_app_reviews_2025)
View(filtro_rating)
View(filtro_review_text)
multilingual_mobile_app_reviews_2025 <- read_csv("multilingual_mobile_app_reviews_2025_extended.csv")
=======
# --- Configuraci√≥n Azure ---
load_dot_env()
>>>>>>> Stashed changes
=======
# --- Configuraci√≥n Azure ---
load_dot_env()
>>>>>>> Stashed changes
library(readr)
library(dplyr)
library(httr)
library(jsonlite)
library(purrr)
library(rlang)
library(cld2)
library(dotenv)
library(e1071)
library(caret)
library(tm)
<<<<<<< Updated upstream
<<<<<<< Updated upstream
multilingual_mobile_app_reviews_2025 <- read_csv("multilingual_mobile_app_reviews_2025_extended.csv")
View(multilingual_mobile_app_reviews_2025)
# Contar valores vac√≠os en todo el dataset
colSums(is.na(multilingual_mobile_app_reviews_2025))
Copia_Datos_Limpios <- multilingual_mobile_app_reviews_2025
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
select(-review_id, -num_helpful_votes, -user_id, -user_gender, -app_version)
colSums(is.na(Copia_Datos_Limpios))
# Filtro general para cualquier variable
filtro_user_country <- Copia_Datos_Limpios %>% filter(is.na(user_country))
filtro_review_text  <- Copia_Datos_Limpios %>% filter(is.na(review_text))
filtro_rating       <- Copia_Datos_Limpios %>% filter(is.na(rating))
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
mutate(
user_country = case_when(
review_language == "es" ~ "Spain",          # Spanish
review_language == "pt" ~ "Brazil",         # Portuguese
review_language == "ja" ~ "Japan",          # Japanese
review_language == "hi" ~ "India",          # Hindi
review_language == "ko" ~ "South Korea",    # Korean
review_language == "zh" ~ "China",          # Chinese
review_language == "de" ~ "Germany",        # German
review_language == "fr" ~ "France",         # French
review_language == "it" ~ "Italy",          # Italian
review_language == "ru" ~ "Russia",         # Russian
TRUE ~ user_country                         # Keep original if no match
)
)
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
filter(!is.na(rating))
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
filter(!is.na(review_text))
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
filter(!is.na(user_country))
colSums(is.na(Copia_Datos_Limpios)[, c("user_country", "review_text", "rating")])
sapply(Copia_Datos_Limpios,class)
language <- unique(Copia_Datos_Limpios$review_language)
typeof(language)
# Asegurar que review_language sea texto
Copia_Datos_Limpios$review_language <- as.character(Copia_Datos_Limpios$review_language)
# Reemplazar 'zh' por 'zh-Hans' (chino simplificado)
Copia_Datos_Limpios$review_language[Copia_Datos_Limpios$review_language == "zh"] <- "zh-Hans"
# Verificar valores √∫nicos despu√©s de la correcci√≥n
unique(Copia_Datos_Limpios$review_language)
=======
=======
>>>>>>> Stashed changes
library(httr)
library(httr)
# --- Configuraci√≥n Azure ---
load_dot_env()
key <- Sys.getenv("AZURE_KEY")
endpoint <- "https://api.cognitive.microsofttranslator.com/"
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
library(jsonlite)
library(httr)
translate_review_text <- function(text, target = "en", key, endpoint, region = "global") {
print(text)
if (is.na(text) || nchar(text) == 0) print("Value null")
url <- paste0(endpoint, "translate?api-version=3.0&to=", target)
body <- list(list(Text = text))
response <- POST(
url,
add_headers(
`Ocp-Apim-Subscription-Key` = key,
`Ocp-Apim-Subscription-Region` = region,
`Content-Type` = "application/json"
),
body = toJSON(body, auto_unbox = TRUE)
)
# Manejo de errores
if (status_code(response) != 200) {
warning(paste("Error:", content(response, "text", encoding = "UTF-8")))
return(NA_character_)
}
result <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
# cat("üì¶ Resultado bruto de la API:\n",
#   jsonlite::toJSON(result, pretty = TRUE, auto_unbox = TRUE),
#   "\n\n")
# str(result)
translated_text <- result$translations[[1]]$text
# cat("‚úÖ Traducido:", translated_text, "\n\n--- Fin impresi√≥n OK ---\n\n")
return(translated_text)
}
# --- Configuraci√≥n Azure ---
load_dot_env()
key <- Sys.getenv("AZURE_KEY")
endpoint <- "https://api.cognitive.microsofttranslator.com/"
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
translate_review_text <- function(text, target = "en", key, endpoint, region = "global") {
print(text)
if (is.na(text) || nchar(text) == 0) print("Value null")
url <- paste0(endpoint, "translate?api-version=3.0&to=", target)
body <- list(list(Text = text))
response <- httr::POST(
url,
httr::add_headers(
`Ocp-Apim-Subscription-Key` = key,
`Ocp-Apim-Subscription-Region` = region,
`Content-Type` = "application/json"
),
body = jsonlite::toJSON(body, auto_unbox = TRUE)
)
# Manejo de errores
if (httr::status_code(response) != 200) {
warning(paste("Error:", httr::content(response, "text", encoding = "UTF-8")))
return(NA_character_)
}
result <- jsonlite::fromJSON(httr::content(response, as = "text", encoding = "UTF-8"))
translated_text <- result$translations[[1]]$text
return(translated_text)
}
# --- Configuraci√≥n Azure ---
load_dot_env()
key <- Sys.getenv("AZURE_KEY")
endpoint <- "https://api.cognitive.microsofttranslator.com/"
region <- "global"
# --- Subconjunto de prueba ---
# Copia_Datos_Limpios_test <- head(Copia_Datos_Limpios, 10)
# --- Aplicar traducci√≥n solo sobre las columnas necesarias ---
Copia_Datos_Limpios$translated <- pmap_chr(
list(Copia_Datos_Limpios$review_text),
function(review_text) {
response <- translate_review_text(
text = review_text,
target = "en",
key = key,
endpoint = endpoint,
region = region
)
if (is.null(response) || length(response) == 0) {
return(NA_character_)
} else {
print(response)  # para ver progreso
return(enc2utf8(as.character(response)))
}
}
)
# Funci√≥n para obtener el idioma t√≠pico de un pa√≠s
get_language_from_country <- function(country) {
# Mapeo de pa√≠ses a c√≥digos de idioma (basado en tu lista)
country_to_language <- c(
"China" = "zh-Hans",          # Chino simplificado
"Russia" = "ru",              # Ruso
"Spain" = "es",               # Espa√±ol
"India" = "hi",               # Hindi (tambi√©n podr√≠a ser "en" si se usa ingl√©s com√∫nmente)
"South Korea" = "ko",         # Coreano
"Australia" = "en",           # Ingl√©s
"Japan" = "ja",               # Japon√©s
"France" = "fr",              # Franc√©s
"Italy" = "it",               # Italiano
"Germany" = "de",             # Alem√°n
"Vietnam" = "vi",             # Vietnamita
"Turkey" = "tr",              # Turco
"Thailand" = "th",            # Tailand√©s
"Brazil" = "pt",              # Portugu√©s
"Nigeria" = "en",             # Ingl√©s (oficial y ampliamente usado)
"United States" = "en",       # Ingl√©s
"Bangladesh" = "bn",          # Bengal√≠
"Canada" = "en",              # Ingl√©s (tambi√©n podr√≠a ser "fr" para franc√©s en Quebec)
"Indonesia" = "id",           # Indonesio
"Mexico" = "es",              # Espa√±ol
"Malaysia" = "ms",            # Malayo
"Pakistan" = "ur",            # Urdu (tambi√©n podr√≠a ser "en" si se usa ingl√©s com√∫nmente)
"Philippines" = "fil",        # Filipino
"United Kingdom" = "en"       # Ingl√©s
)
# Devolver el idioma correspondiente al pa√≠s
return(country_to_language[country])
}
detect_language <- function(text) {
# Asegurarse de que sea texto
text <- as.character(text)
text[is.na(text)] <- ""
# Si est√° vac√≠o, asignar NA
empty <- nchar(trimws(text)) == 0
result <- rep(NA_character_, length(text))
# Detectar idioma solo donde haya texto
if (any(!empty)) {
result[!empty] <- cld2::detect_language(text[!empty])
}
# Detectar patrones t√≠picos de Lorem Ipsum
lorem_pattern <- "(?i)\\blorem\\b|\\bipsum\\b|\\bdolor\\b|\\bamet\\b|\\bconsectetur\\b|\\badipiscing\\b|\\belit\\b|\\btempor\\b|\\blabore\\b|\\bnemo\\b|\\bhic\\b|\\bunde\\b|\\bvoluptate\\b|\\bcommodi\\b|\\bfacilis\\b|\\bsuscipit\\b|\\bquia\\b|\\btempora\\b|\\bexcepturi\\b|\\bdeleniti\\b"
# Marcar como 'la' si contiene palabras de Lorem Ipsum
result[grepl(lorem_pattern, text)] <- "la"
# Reemplazar NAs (no detectados) tambi√©n por 'la'
result[is.na(result)] <- "la"
return(result)
}
# Funci√≥n corregida y vectorizada para evaluar si un texto es v√°lido
is_valid_review <- function(text) {
# Asegurarse de que sea car√°cter
text <- as.character(text)
# Reemplazar NA por cadena vac√≠a para evitar errores
text[is.na(text)] <- ""
# Eliminar espacios y pasar a min√∫sculas
text <- tolower(trimws(text))
# Texto vac√≠o ‚Üí FALSE
empty <- nchar(text) == 0
# Crear un solo patr√≥n con todas las palabras v√°lidas
pattern <- paste0("\\b(", paste(valid_words, collapse = "|"), ")\\b")
# Detectar si contiene alguna palabra v√°lida
has_valid_word <- grepl(pattern, text)
# Resultado final
result <- !empty & has_valid_word
return(result)
}
# Bolsa de palabras v√°lidas para rese√±as
valid_words <- c(
"great", "good", "excellent", "amazing", "love", "helpful", "easy", "fast",
"quality", "recommend", "happy", "satisfied", "improved", "useful", "perfect",
"bad", "terrible", "disappointed", "problem", "issue", "broken", "slow",
"update", "feature", "service", "support", "interface", "design", "experience",
"download", "install", "upgrade", "bug", "error", "crash", "fix", "solution"
)
# Evaluaci√≥n del dataset
Copia_Datos_Limpios_classified <- Copia_Datos_Limpios %>%
mutate(
spam_flag = case_when(
detect_language(translated) == "en" & is_valid_review(translated) ~ "ham",   # ingl√©s y v√°lido
detect_language(translated) == "en" & !is_valid_review(translated) ~ "spam", # ingl√©s pero no v√°lido
TRUE ~ "spam"  # todo lo dem√°s (no ingl√©s, NA, etc.) se marca como spam
)
)
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
=======
>>>>>>> Stashed changes
# Crear la variable spam_flag seg√∫n duplicados en 'translated'
Copia_Datos_Limpios_classified <- Copia_Datos_Limpios_classified %>%
mutate(
spam_flag = ifelse(
duplicated(translated) | duplicated(translated, fromLast = TRUE),
"spam",
"ham"
)
)
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
mutate(
detected_language = sapply(review_text, detect_language), # Detectar el idioma del texto
spam_verified = case_when(
spam_verified == "ham" &
(
review_language != get_language_from_country(user_country) | # Idioma declarado no coincide con el pa√≠s
detected_language != get_language_from_country(user_country) # Idioma detectado no coincide con el pa√≠s
) ~ "spam", # Marcar como spam si no coincide el idioma
TRUE ~ spam_verified # Mantener el valor original si no se cumplen las condiciones
)
) %>%
select(-detected_language) # Eliminar columna auxiliar si no es necesaria
detect_language("Eius odio facilis fuga distinctio eaque.")
is_latin <- function(text) {
# Asegurarse de que sea texto
text <- as.character(text)
text[is.na(text)] <- ""
# Detectar idioma usando la funci√≥n anterior
detected_lang <- detect_language(text)
# Lista de palabras clave y sufijos t√≠picos del lat√≠n
latin_keywords <- c(
"quis", "doloribus", "consequuntur", "perspiciatis",
"tempora", "assumenda", "atque", "doloremque",
"nobis", "voluptatem", "quidem", "esse"
)
latin_suffixes <- c("us", "is", "um", "ae", "am", "ibus", "i", "o")
# Convertir texto a min√∫sculas
text_lower <- tolower(text)
# Buscar palabras y sufijos latinos
has_keywords <- sapply(text_lower, function(txt) any(sapply(latin_keywords, grepl, txt)))
has_suffixes <- sapply(text_lower, function(txt) any(sapply(latin_suffixes, function(s) grepl(paste0("\\b\\w*", s, "\\b"), txt))))
# Marcar TRUE si el idioma detectado fue 'la' o si cumple los patrones
return(detected_lang == "la" | has_keywords | has_suffixes)
}
is_latin("Nulla ullam quo iure repudiandae.")
Copia_Datos_Limpios_classified <- Copia_Datos_Limpios_classified %>%
mutate(
spam_flag = case_when(
spam_flag == "ham" & is_latin(review_text) ~ "spam", # Marcar como spam si es lat√≠n
TRUE ~ spam_flag # Mantener el valor original si no se cumplen las condiciones
)
)
unique(Copia_Datos_Limpios_classified$spam_verified)
values_verified <- Copia_Datos_Limpios_classified %>% filter(spam_flag == "ham")
View(values_verified)
Copia_Datos_Limpios_classified$translated <- as.factor(Copia_Datos_Limpios_classified$translated)
Copia_Datos_Limpios_classified$spam_flag <- as.factor(Copia_Datos_Limpios_classified$spam_flag)
View(Copia_Datos_Limpios_classified)
text_data <- Copia_Datos_Limpios_classified[, c("translated", "spam_flag")]
corpus <- VCorpus(VectorSource(text_data$translated))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stripWhitespace)
dtm_full <- DocumentTermMatrix(corpus)
cat("Dimensiones DTM inicial (docs x t√©rminos):", dim(dtm_full), "\n")
dtm_reduced <- removeSparseTerms(dtm_full, 0.99)
cat("Dimensiones DTM reducidas", dim(dtm_reduced), "\n")
set.seed(123)
index <- createDataPartition(text_data$spam_flag, p = 0.7, list = FALSE)
train_dtm_mat <- as.matrix(dtm_reduced)[index, , drop=FALSE]
test_dtm_mat <- as.matrix(dtm_reduced)[-index, , drop=FALSE]
train_labels <- as.factor(text_data$spam_flag[index])
test_labels <- as.factor(text_data$spam_flag)[-index]
cat("Train:", dim(train_dtm_mat), "Test", dim(test_labels), "\n")
train_bin <- ifelse(train_dtm_mat > 0, 1, 0)
test_bin  <- ifelse(test_dtm_mat > 0, 1, 0)
test_bin <- test_bin[, colnames(train_bin), drop = FALSE]
train_df <- as.data.frame(lapply(as.data.frame(train_bin), as.numeric))
test_df <- as.data.frame(lapply(as.data.frame(test_bin), as.numeric))
cat("train_df:", dim(train_df), " test_df:", dim(test_df), "\n")
modelo_nb <- naiveBayes(x = train_df, y = train_labels)
predecir <- predict(modelo_nb, newdata = test_df)
cat("\nMatriz de Confusi√≥n:\n")
print(table(Predicci√≥n = predecir, Real = test_labels))
# M√©tricas principales con caret
confusion <- confusionMatrix(predecir, test_labels)
print(confusion)
accuracy <- confusion$overall['Accuracy']
precision <- confusion$byClass['Pos Pred Value']
recall <- confusion$byClass['Sensitivity']
f1 <- 2 * ((precision * recall) / (precision + recall))
print(accuracy)
print(precision)
print(recall)
print(f1)
#Convertimos etiquetas a 0/1
true_numeric <- ifelse(test_labels == "spam", 1, 0)
pred_numeric <- ifelse(predecir == "spam", 1, 0)
mae <- mean(abs(true_numeric - pred_numeric))
print(mae)
cat("\n M√©tricas de Evaluaci√≥n \n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Precision:", round(precision * 100, 2), "%\n")
cat("Recall:", round(recall * 100, 2), "%\n")
cat("F1-Score:", round(f1 * 100, 2), "%\n")
cat("MAE:", round(mae, 4), "\n")
rm(Copia_Datos_Limpios_classified, multilingual_mobile_app_reviews_2025_extended)
# Evaluaci√≥n del dataset
Copia_Datos_Limpios_classified <- Copia_Datos_Limpios %>%
mutate(
spam_flag = case_when(
detect_language(translated) == "en" & is_valid_review(translated) ~ "ham",   # ingl√©s y v√°lido
detect_language(translated) == "en" & !is_valid_review(translated) ~ "spam", # ingl√©s pero no v√°lido
TRUE ~ "spam"  # todo lo dem√°s (no ingl√©s, NA, etc.) se marca como spam
)
)
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
Copia_Datos_Limpios_classified$translated <- as.factor(Copia_Datos_Limpios_classified$translated)
Copia_Datos_Limpios_classified$spam_flag <- as.factor(Copia_Datos_Limpios_classified$spam_flag)
text_data <- Copia_Datos_Limpios_classified[, c("translated", "spam_flag")]
corpus <- VCorpus(VectorSource(text_data$translated))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stripWhitespace)
dtm_full <- DocumentTermMatrix(corpus)
cat("Dimensiones DTM inicial (docs x t√©rminos):", dim(dtm_full), "\n")
dtm_reduced <- removeSparseTerms(dtm_full, 0.99)
cat("Dimensiones DTM reducidas", dim(dtm_reduced), "\n")
set.seed(123)
index <- createDataPartition(text_data$spam_flag, p = 0.7, list = FALSE)
train_dtm_mat <- as.matrix(dtm_reduced)[index, , drop=FALSE]
test_dtm_mat <- as.matrix(dtm_reduced)[-index, , drop=FALSE]
train_labels <- as.factor(text_data$spam_flag[index])
test_labels <- as.factor(text_data$spam_flag)[-index]
cat("Train:", dim(train_dtm_mat), "Test", dim(test_labels), "\n")
train_bin <- ifelse(train_dtm_mat > 0, 1, 0)
test_bin  <- ifelse(test_dtm_mat > 0, 1, 0)
test_bin <- test_bin[, colnames(train_bin), drop = FALSE]
train_df <- as.data.frame(lapply(as.data.frame(train_bin), as.numeric))
test_df <- as.data.frame(lapply(as.data.frame(test_bin), as.numeric))
cat("train_df:", dim(train_df), " test_df:", dim(test_df), "\n")
modelo_nb <- naiveBayes(x = train_df, y = train_labels)
predecir <- predict(modelo_nb, newdata = test_df)
cat("\nMatriz de Confusi√≥n:\n")
print(table(Predicci√≥n = predecir, Real = test_labels))
# M√©tricas principales con caret
confusion <- confusionMatrix(predecir, test_labels)
print(confusion)
accuracy <- confusion$overall['Accuracy']
precision <- confusion$byClass['Pos Pred Value']
recall <- confusion$byClass['Sensitivity']
f1 <- 2 * ((precision * recall) / (precision + recall))
print(accuracy)
print(precision)
print(recall)
print(f1)
#Convertimos etiquetas a 0/1
true_numeric <- ifelse(test_labels == "spam", 1, 0)
pred_numeric <- ifelse(predecir == "spam", 1, 0)
mae <- mean(abs(true_numeric - pred_numeric))
print(mae)
cat("\n M√©tricas de Evaluaci√≥n \n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Precision:", round(precision * 100, 2), "%\n")
cat("Recall:", round(recall * 100, 2), "%\n")
cat("F1-Score:", round(f1 * 100, 2), "%\n")
cat("MAE:", round(mae, 4), "\n")
<<<<<<< Updated upstream
<<<<<<< Updated upstream
boxplot(data_reviews$rating, main = "Boxplot de Rating")
boxplot(Copia_Datos_Limpios$rating, main = "Boxplot de Rating")
View(Copia_Datos_Limpios)
boxplot(Copia_Datos_Limpios$rating, main = "Boxplot de Rating")
boxplot(Copia_Datos_Limpios$rating, main = "Boxplot de Rating")
boxplot(Copia_Datos_Limpios$review_text, main = "Boxplot de Rating")
boxplot(Copia_Datos_Limpios$user_age, main = "Boxplot de Rating")
View(Copia_Datos_Limpios_classified)
View(multilingual_mobile_app_reviews_2025)
View(index)
View(train_dtm_mat)
View(Copia_Datos_Limpios_classified)
write.csv(Copia_Datos_Limpios_classified, file = "copia_datos.csv")
View(Copia_Datos_Limpios)
View(Copia_Datos_Limpios)
colnames(Copia_Datos_Limpios_classified)
head(Copia_Datos_Limpios_classified, 2)
head(Copia_Datos_Limpios_classified, 5)
head(Copia_Datos_Limpios_classified, 2)
head(Copia_Datos_Limpios_classified, 1)
head(Copia_Datos_Limpios_classified, 1)
print(head)
View(Copia_Datos_Limpios_classified)
=======
=======
>>>>>>> Stashed changes
unique(Copia_Datos_Limpios_classified$spam_verified)
values_verified <- Copia_Datos_Limpios_classified %>% filter(spam_flag == "ham")
View(values_verified)
View(test_bin)
View(test_dtm_mat)
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
