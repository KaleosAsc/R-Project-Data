empty <- nchar(trimws(text)) == 0
result <- rep(NA_character_, length(text))
if (any(!empty)) {
result[!empty] <- cld2::detect_language(text[!empty])
}
# Detectar patrones de Lorem Ipsum
lorem_pattern <- "(?i)\\blorem\\b|\\bipsum\\b|\\bdolor\\b|\\bamet\\b|\\bconsectetur\\b|\\badipiscing\\b|\\belit\\b"
result[grepl(lorem_pattern, text)] <- "la"
result[is.na(result)] <- "la"
return(result)
}
# Bolsa de palabras válidas para reseñas
valid_words <- c(
"great", "good", "excellent", "amazing", "love", "helpful", "easy", "fast",
"quality", "recommend", "happy", "satisfied", "improved", "useful", "perfect",
"bad", "terrible", "disappointed", "problem", "issue", "broken", "slow",
"update", "feature", "service", "support", "interface", "design", "experience",
"download", "install", "upgrade", "bug", "error", "crash", "fix", "solution"
)
# Función para evaluar si un texto es válido
is_valid_review <- function(text) {
text <- as.character(text)
text[is.na(text)] <- ""
text <- tolower(trimws(text))
empty <- nchar(text) == 0
pattern <- paste0("\\b(", paste(valid_words, collapse = "|"), ")\\b")
has_valid_word <- grepl(pattern, text)
result <- !empty & has_valid_word
return(result)
}
# Cargar datos traducidos y clasificar reseñas
Copia_Datos_Limpios <- readRDS("cache_traduccion.rds")
Copia_Datos_Limpios_classified <- Copia_Datos_Limpios %>%
mutate(
spam_flag = case_when(
detect_language(translated) == "en" & is_valid_review(translated) ~ "ham",
detect_language(translated) == "en" & !is_valid_review(translated) ~ "spam",
TRUE ~ "spam"
)
)
# Convertir variables a factor
Copia_Datos_Limpios_classified$translated <- as.factor(Copia_Datos_Limpios_classified$translated)
Copia_Datos_Limpios_classified$spam_flag <- as.factor(Copia_Datos_Limpios_classified$spam_flag)
# Subconjunto con texto y etiqueta
text_data <- Copia_Datos_Limpios_classified[, c("translated", "spam_flag")]
# Crear y limpiar corpus
corpus <- VCorpus(VectorSource(text_data$translated))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stripWhitespace)
# Crear matriz de términos (DTM)
dtm_full <- DocumentTermMatrix(corpus)
cat("Dimensiones DTM inicial (docs x términos):", dim(dtm_full), "\n")
# Reducir términos muy raros
dtm_reduced <- removeSparseTerms(dtm_full, 0.99)
cat("Dimensiones DTM reducidas:", dim(dtm_reduced), "\n")
set.seed(123)
index <- createDataPartition(text_data$spam_flag, p = 0.7, list = FALSE)
train_dtm_mat <- as.matrix(dtm_reduced)[index, , drop=FALSE]
test_dtm_mat <- as.matrix(dtm_reduced)[-index, , drop=FALSE]
train_labels <- as.factor(text_data$spam_flag[index])
test_labels <- as.factor(text_data$spam_flag)[-index]
cat("Train:", dim(train_dtm_mat), "Test:", length(test_labels), "\n")
# Binarizar (0/1 si la palabra aparece)
train_bin <- ifelse(train_dtm_mat > 0, 1, 0)
test_bin  <- ifelse(test_dtm_mat > 0, 1, 0)
# Alinear columnas
test_bin <- test_bin[, colnames(train_bin), drop = FALSE]
# Convertir a dataframe numérico
train_df <- as.data.frame(lapply(as.data.frame(train_bin), as.numeric))
test_df <- as.data.frame(lapply(as.data.frame(test_bin), as.numeric))
cat("train_df:", dim(train_df), " test_df:", dim(test_df), "\n")
# Entrenar modelo Naive Bayes
modelo_nb <- naiveBayes(x = train_df, y = train_labels)
# Predecir en conjunto de prueba
predecir <- predict(modelo_nb, newdata = test_df)
train_dict <- colnames(train_dtm_mat)
prediction_fun <- function(texto) {
# Crear corpus con el nuevo texto
corpus_nuevo <- Corpus(VectorSource(texto))
corpus_nuevo <- tm_map(corpus_nuevo, content_transformer(tolower))
corpus_nuevo <- tm_map(corpus_nuevo, removePunctuation)
corpus_nuevo <- tm_map(corpus_nuevo, removeNumbers)
corpus_nuevo <- tm_map(corpus_nuevo, removeWords, stopwords("english"))
# Crear DTM usando el mismo diccionario del entrenamiento
dtm_nuevo <- DocumentTermMatrix(corpus_nuevo,
control = list(dictionary = train_dict))
# Binarizar
nuevo_bin <- ifelse(as.matrix(dtm_nuevo) > 0, 1, 0)
# Convertir a data.frame numérico
nuevo_df <- as.data.frame(lapply(as.data.frame(nuevo_bin), as.numeric))
# Alinear columnas
for (col in setdiff(colnames(train_df), colnames(nuevo_df))) {
nuevo_df[[col]] <- 0
}
nuevo_df <- nuevo_df[, colnames(train_df), drop = FALSE]
# Predecir
pred <- predict(modelo_nb, newdata = nuevo_df)
return(as.character(pred))
}
# Matriz de confusión
cat("\nMatriz de Confusión:\n")
print(table(Predicción = predecir, Real = test_labels))
# Métricas con caret
confusion <- confusionMatrix(predecir, test_labels)
# Métricas individuales
accuracy <- confusion$overall['Accuracy']
precision <- confusion$byClass['Pos Pred Value']
recall <- confusion$byClass['Sensitivity']
f1 <- 2 * ((precision * recall) / (precision + recall))
# Calcular MAE
true_numeric <- ifelse(test_labels == "spam", 1, 0)
pred_numeric <- ifelse(predecir == "spam", 1, 0)
mae <- mean(abs(true_numeric - pred_numeric))
# Mostrar resultados
cat("\n=== MÉTRICAS DE EVALUACIÓN ===\n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Precision:", round(precision * 100, 2), "%\n")
cat("Recall:", round(recall * 100, 2), "%\n")
cat("F1-Score:", round(f1 * 100, 2), "%\n")
cat("MAE:", round(mae, 4), "\n")
# Matriz de confusión como heatmap
cm <- as.data.frame(confusion$table)
p1 <- ggplot(data = cm, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 5, color = "black") +
scale_fill_gradient(low = "#EAF2F8", high = "#2E86C1") +
labs(
title = "Matriz de Confusión - Modelo Naive Bayes",
x = "Etiqueta Real",
y = "Predicción del Modelo"
) +
theme_minimal(base_size = 12)
# Métricas comparativas
metricas <- data.frame(
Metrica = c("Accuracy", "Precision", "Recall", "F1-Score"),
Valor = c(accuracy, precision, recall, f1)
)
p2 <- ggplot(metricas, aes(x = Metrica, y = Valor, fill = Metrica)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = round(Valor, 3)), vjust = -0.5, size = 4) +
scale_fill_brewer(palette = "Blues") +
ylim(0, 1) +
labs(
title = "Métricas de Evaluación del Modelo",
y = "Valor (0–1)"
) +
theme_minimal(base_size = 12) +
theme(legend.position = "none")
print(p1)
print(p2)
# Matriz de confusión como heatmap
cm <- as.data.frame(confusion$table)
p1 <- ggplot(data = cm, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 5, color = "black") +
scale_fill_gradient(low = "#EAF2F8", high = "#2E86C1") +
labs(
title = "Matriz de Confusión - Modelo Naive Bayes",
x = "Etiqueta Real",
y = "Predicción del Modelo"
) +
theme_minimal(base_size = 12)
# Métricas comparativas
metricas <- data.frame(
Metrica = c("Accuracy", "Precision", "Recall", "F1-Score"),
Valor = c(accuracy, precision, recall, f1)
)
p2 <- ggplot(metricas, aes(x = Metrica, y = Valor, fill = Metrica)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = round(Valor, 3)), vjust = -0.5, size = 4) +
scale_fill_brewer(palette = "Blues") +
ylim(0, 1) +
labs(
title = "Métricas de Evaluación del Modelo",
y = "Valor (0–1)"
) +
theme_minimal(base_size = 12) +
theme(legend.position = "none")
print(p1)
print(p2)
# Entrenar modelo Naive Bayes
modelo_nb <- naiveBayes(x = train_df, y = train_labels)
# Predecir en conjunto de prueba
predecir <- predict(modelo_nb, newdata = test_df)
train_dict <- colnames(train_dtm_mat)
prediction_fun <- function(texto) {
# Crear corpus con el nuevo texto
corpus_nuevo <- Corpus(VectorSource(texto))
corpus_nuevo <- tm_map(corpus_nuevo, content_transformer(tolower))
corpus_nuevo <- tm_map(corpus_nuevo, removePunctuation)
corpus_nuevo <- tm_map(corpus_nuevo, removeNumbers)
corpus_nuevo <- tm_map(corpus_nuevo, removeWords, stopwords("english"))
# Crear DTM usando el mismo diccionario del entrenamiento
dtm_nuevo <- DocumentTermMatrix(corpus_nuevo,
control = list(dictionary = train_dict))
# Binarizar
nuevo_bin <- ifelse(as.matrix(dtm_nuevo) > 0, 1, 0)
# Convertir a data.frame numérico
nuevo_df <- as.data.frame(lapply(as.data.frame(nuevo_bin), as.numeric))
# Alinear columnas
for (col in setdiff(colnames(train_df), colnames(nuevo_df))) {
nuevo_df[[col]] <- 0
}
nuevo_df <- nuevo_df[, colnames(train_df), drop = FALSE]
# Predecir
pred <- predict(modelo_nb, newdata = nuevo_df)
return(as.character(pred))
}
# Matriz de confusión
cat("\nMatriz de Confusión:\n")
print(table(Predicción = predecir, Real = test_labels))
# Métricas con caret
confusion <- confusionMatrix(predecir, test_labels)
# Métricas individuales
accuracy <- confusion$overall['Accuracy']
precision <- confusion$byClass['Pos Pred Value']
recall <- confusion$byClass['Sensitivity']
f1 <- 2 * ((precision * recall) / (precision + recall))
# Calcular MAE
true_numeric <- ifelse(test_labels == "spam", 1, 0)
pred_numeric <- ifelse(predecir == "spam", 1, 0)
mae <- mean(abs(true_numeric - pred_numeric))
# Mostrar resultados
cat("\n=== MÉTRICAS DE EVALUACIÓN ===\n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Precision:", round(precision * 100, 2), "%\n")
cat("Recall:", round(recall * 100, 2), "%\n")
cat("F1-Score:", round(f1 * 100, 2), "%\n")
cat("MAE:", round(mae, 4), "\n")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Instalación de paquetes (ejecutar solo si es necesario)
# install.packages(c("httr", "jsonlite", "cld2", "dotenv", "tm", "e1071", "caret", "reticulate"))
library(readr)
library(dplyr)
library(httr)
library(jsonlite)
library(purrr)
library(rlang)
library(cld2)
library(dotenv)
library(e1071)
library(caret)
library(tm)
library(reticulate)
library(ggplot2)
multilingual_mobile_app_reviews_2025 <- read_csv("multilingual_mobile_app_reviews_2025_extended.csv")
Copia_Datos_Limpios <- multilingual_mobile_app_reviews_2025 %>%
select(-review_id, -num_helpful_votes, -user_id, -user_gender, -app_version)
# Contar valores vacíos en todo el dataset
cat("Valores faltantes por columna:\n")
colSums(is.na(Copia_Datos_Limpios))
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
mutate(
user_country = case_when(
review_language == "es" ~ "Spain",
review_language == "pt" ~ "Brazil",
review_language == "ja" ~ "Japan",
review_language == "hi" ~ "India",
review_language == "ko" ~ "South Korea",
review_language == "zh" ~ "China",
review_language == "de" ~ "Germany",
review_language == "fr" ~ "France",
review_language == "it" ~ "Italy",
review_language == "ru" ~ "Russia",
TRUE ~ user_country
)
)
Copia_Datos_Limpios <- Copia_Datos_Limpios %>%
filter(!is.na(rating)) %>%
filter(!is.na(review_text)) %>%
filter(!is.na(user_country))
cat("\nValores faltantes después de limpieza:\n")
colSums(is.na(Copia_Datos_Limpios))
# Asegurar que review_language sea texto
Copia_Datos_Limpios$review_language <- as.character(Copia_Datos_Limpios$review_language)
# Reemplazar 'zh' por 'zh-Hans' (chino simplificado)
Copia_Datos_Limpios$review_language[Copia_Datos_Limpios$review_language == "zh"] <- "zh-Hans"
translate_review_text <- function(text, target = "en", key, endpoint, region = "global") {
if (is.na(text) || nchar(text) == 0) return(NA_character_)
url <- paste0(endpoint, "translate?api-version=3.0&to=", target)
body <- list(list(Text = text))
response <- httr::POST(
url,
httr::add_headers(
`Ocp-Apim-Subscription-Key` = key,
`Ocp-Apim-Subscription-Region` = region,
`Content-Type` = "application/json"
),
body = jsonlite::toJSON(body, auto_unbox = TRUE)
)
if (httr::status_code(response) != 200) {
warning(paste("Error:", httr::content(response, "text", encoding = "UTF-8")))
return(NA_character_)
}
result <- jsonlite::fromJSON(httr::content(response, as = "text", encoding = "UTF-8"))
return(result$translations[[1]]$text)
}
# Función detector de lenguaje
detect_language <- function(text) {
text <- as.character(text)
text[is.na(text)] <- ""
empty <- nchar(trimws(text)) == 0
result <- rep(NA_character_, length(text))
if (any(!empty)) {
result[!empty] <- cld2::detect_language(text[!empty])
}
# Detectar patrones de Lorem Ipsum
lorem_pattern <- "(?i)\\blorem\\b|\\bipsum\\b|\\bdolor\\b|\\bamet\\b|\\bconsectetur\\b|\\badipiscing\\b|\\belit\\b"
result[grepl(lorem_pattern, text)] <- "la"
result[is.na(result)] <- "la"
return(result)
}
# Bolsa de palabras válidas para reseñas
valid_words <- c(
"great", "good", "excellent", "amazing", "love", "helpful", "easy", "fast",
"quality", "recommend", "happy", "satisfied", "improved", "useful", "perfect",
"bad", "terrible", "disappointed", "problem", "issue", "broken", "slow",
"update", "feature", "service", "support", "interface", "design", "experience",
"download", "install", "upgrade", "bug", "error", "crash", "fix", "solution"
)
# Función para evaluar si un texto es válido
is_valid_review <- function(text) {
text <- as.character(text)
text[is.na(text)] <- ""
text <- tolower(trimws(text))
empty <- nchar(text) == 0
pattern <- paste0("\\b(", paste(valid_words, collapse = "|"), ")\\b")
has_valid_word <- grepl(pattern, text)
result <- !empty & has_valid_word
return(result)
}
# Cargar datos traducidos y clasificar reseñas
Copia_Datos_Limpios <- readRDS("cache_traduccion.rds")
Copia_Datos_Limpios_classified <- Copia_Datos_Limpios %>%
mutate(
spam_flag = case_when(
detect_language(translated) == "en" & is_valid_review(translated) ~ "ham",
detect_language(translated) == "en" & !is_valid_review(translated) ~ "spam",
TRUE ~ "spam"
)
)
# Convertir variables a factor
Copia_Datos_Limpios_classified$translated <- as.factor(Copia_Datos_Limpios_classified$translated)
Copia_Datos_Limpios_classified$spam_flag <- as.factor(Copia_Datos_Limpios_classified$spam_flag)
# Subconjunto con texto y etiqueta
text_data <- Copia_Datos_Limpios_classified[, c("translated", "spam_flag")]
# Crear y limpiar corpus
corpus <- VCorpus(VectorSource(text_data$translated))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stripWhitespace)
# Crear matriz de términos (DTM)
dtm_full <- DocumentTermMatrix(corpus)
cat("Dimensiones DTM inicial (docs x términos):", dim(dtm_full), "\n")
# Reducir términos muy raros
dtm_reduced <- removeSparseTerms(dtm_full, 0.99)
cat("Dimensiones DTM reducidas:", dim(dtm_reduced), "\n")
set.seed(123)
index <- createDataPartition(text_data$spam_flag, p = 0.7, list = FALSE)
train_dtm_mat <- as.matrix(dtm_reduced)[index, , drop=FALSE]
test_dtm_mat <- as.matrix(dtm_reduced)[-index, , drop=FALSE]
train_labels <- as.factor(text_data$spam_flag[index])
test_labels <- as.factor(text_data$spam_flag)[-index]
cat("Train:", dim(train_dtm_mat), "Test:", length(test_labels), "\n")
# Binarizar (0/1 si la palabra aparece)
train_bin <- ifelse(train_dtm_mat > 0, 1, 0)
test_bin  <- ifelse(test_dtm_mat > 0, 1, 0)
# Alinear columnas
test_bin <- test_bin[, colnames(train_bin), drop = FALSE]
# Convertir a dataframe numérico
train_df <- as.data.frame(lapply(as.data.frame(train_bin), as.numeric))
test_df <- as.data.frame(lapply(as.data.frame(test_bin), as.numeric))
cat("train_df:", dim(train_df), " test_df:", dim(test_df), "\n")
# Entrenar modelo Naive Bayes
modelo_nb <- naiveBayes(x = train_df, y = train_labels)
# Predecir en conjunto de prueba
predecir <- predict(modelo_nb, newdata = test_df)
train_dict <- colnames(train_dtm_mat)
prediction_fun <- function(texto) {
# Crear corpus con el nuevo texto
corpus_nuevo <- Corpus(VectorSource(texto))
corpus_nuevo <- tm_map(corpus_nuevo, content_transformer(tolower))
corpus_nuevo <- tm_map(corpus_nuevo, removePunctuation)
corpus_nuevo <- tm_map(corpus_nuevo, removeNumbers)
corpus_nuevo <- tm_map(corpus_nuevo, removeWords, stopwords("english"))
# Crear DTM usando el mismo diccionario del entrenamiento
dtm_nuevo <- DocumentTermMatrix(corpus_nuevo,
control = list(dictionary = train_dict))
# Binarizar
nuevo_bin <- ifelse(as.matrix(dtm_nuevo) > 0, 1, 0)
# Convertir a data.frame numérico
nuevo_df <- as.data.frame(lapply(as.data.frame(nuevo_bin), as.numeric))
# Alinear columnas
for (col in setdiff(colnames(train_df), colnames(nuevo_df))) {
nuevo_df[[col]] <- 0
}
nuevo_df <- nuevo_df[, colnames(train_df), drop = FALSE]
# Predecir
pred <- predict(modelo_nb, newdata = nuevo_df)
return(as.character(pred))
}
# Matriz de confusión
cat("\nMatriz de Confusión:\n")
print(table(Predicción = predecir, Real = test_labels))
# Métricas con caret
confusion <- confusionMatrix(predecir, test_labels)
# Métricas individuales
accuracy <- confusion$overall['Accuracy']
precision <- confusion$byClass['Pos Pred Value']
recall <- confusion$byClass['Sensitivity']
f1 <- 2 * ((precision * recall) / (precision + recall))
# Calcular MAE
true_numeric <- ifelse(test_labels == "spam", 1, 0)
pred_numeric <- ifelse(predecir == "spam", 1, 0)
mae <- mean(abs(true_numeric - pred_numeric))
# Mostrar resultados
cat("\n=== MÉTRICAS DE EVALUACIÓN ===\n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Precision:", round(precision * 100, 2), "%\n")
cat("Recall:", round(recall * 100, 2), "%\n")
cat("F1-Score:", round(f1 * 100, 2), "%\n")
cat("MAE:", round(mae, 4), "\n")
# Matriz de confusión como heatmap
cm <- as.data.frame(confusion$table)
p1 <- ggplot(data = cm, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 5, color = "black") +
scale_fill_gradient(low = "#EAF2F8", high = "#2E86C1") +
labs(
title = "Matriz de Confusión - Modelo Naive Bayes",
x = "Etiqueta Real",
y = "Predicción del Modelo"
) +
theme_minimal(base_size = 12)
# Métricas comparativas
metricas <- data.frame(
Metrica = c("Accuracy", "Precision", "Recall", "F1-Score"),
Valor = c(accuracy, precision, recall, f1)
)
p2 <- ggplot(metricas, aes(x = Metrica, y = Valor, fill = Metrica)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = round(Valor, 3)), vjust = -0.5, size = 4) +
scale_fill_brewer(palette = "Blues") +
ylim(0, 1) +
labs(
title = "Métricas de Evaluación del Modelo",
y = "Valor (0–1)"
) +
theme_minimal(base_size = 12) +
theme(legend.position = "none")
print(p1)
print(p2)
# Matriz de confusión
cat("\nMatriz de Confusión:\n")
print(table(Predicción = predecir, Real = test_labels))
# Métricas con caret
confusion <- confusionMatrix(predecir, test_labels)
# Métricas individuales
accuracy <- confusion$overall['Accuracy']
precision <- confusion$byClass['Pos Pred Value']
recall <- confusion$byClass['Sensitivity']
f1 <- 2 * ((precision * recall) / (precision + recall))
# Calcular MAE
true_numeric <- ifelse(test_labels == "spam", 1, 0)
pred_numeric <- ifelse(predecir == "spam", 1, 0)
mae <- mean(abs(true_numeric - pred_numeric))
# Mostrar resultados
cat("\n=== MÉTRICAS DE EVALUACIÓN ===\n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Precision:", round(precision * 100, 2), "%\n")
cat("Recall:", round(recall * 100, 2), "%\n")
cat("F1-Score:", round(f1 * 100, 2), "%\n")
cat("MAE:", round(mae, 4), "\n")
# Matriz de confusión como heatmap
cm <- as.data.frame(confusion$table)
p1 <- ggplot(data = cm, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 5, color = "black") +
scale_fill_gradient(low = "#EAF2F8", high = "#2E86C1") +
labs(
title = "Matriz de Confusión - Modelo Naive Bayes",
x = "Etiqueta Real",
y = "Predicción del Modelo"
) +
theme_minimal(base_size = 12)
# Métricas comparativas
metricas <- data.frame(
Metrica = c("Accuracy", "Precision", "Recall", "F1-Score"),
Valor = c(accuracy, precision, recall, f1)
)
p2 <- ggplot(metricas, aes(x = Metrica, y = Valor, fill = Metrica)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = round(Valor, 3)), vjust = -0.5, size = 4) +
scale_fill_brewer(palette = "Blues") +
ylim(0, 1) +
labs(
title = "Métricas de Evaluación del Modelo",
y = "Valor (0–1)"
) +
theme_minimal(base_size = 12) +
theme(legend.position = "none")
print(p1)
print(p2)
# Entrenar modelo Naive Bayes
modelo_nb <- naiveBayes(x = train_df, y = train_labels)
# Predecir en conjunto de prueba
predecir <- predict(modelo_nb, newdata = test_df)
